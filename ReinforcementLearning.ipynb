{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Deep Reinforcement Learning**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this lab, you will get an introduction to reinforcement learning by solving a real-time decision problem.\n",
                "\n",
                "Please open [this tutorial](https://youtu.be/cO5g5qLrLSo) to follow the steps in this notebook. This is a 20 minutes tutorial in which you will learn how to define, train and test a reinforcement learning problem. You will also learn some useful databases where you can download similar problems.\n",
                "\n",
                "\n",
                "**Saturn shortcuts**\n",
                "\n",
                "Press Ctrl+return to run each section separately. Please note that some sections depend on the previous sections, and run them in order. You can run the whole program at once, buy clicking the Run All button.\n",
                "\n",
                "---\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Theory: The four concepts of reinforcement learning\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "***Check point***\n",
                "\n",
                "What are the four main concepts that make up reinforcement learning? (Hint: Area 51)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Action, Reward, Environment, Agent"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 0. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "# All the packages are available in EdStem.\n",
                "# This code prevents multiple installations on the EdStem operating system\n",
                "import os \n",
                "if not os.getenv(\"ED_COURSE_ID\"):\n",
                "    !pip install tensorflow gym keras keras-rl2"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Test Random Environment with OpenAI Gym"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('./.local/lib/python3.9/site-packages')\n",
                "import gym \n",
                "import random"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Set up environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use the make method to generate the CartPole environment and set it to env\n",
                "env = gym.make('CartPole-v0')\n",
                "\n",
                "# Extract the available states and actions\n",
                "states = env.observation_space.shape[0]\n",
                "actions = env.action_space.n\n",
                "\n",
                "#TODO: Write code to inspect the number of actions available in this problem\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Visualize the random environment"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Note on visualization**: If the display window comes up but the environment is not displayed, please click on the \"...\" on the top right of the window to select \"Full view\", then click on the \"Remote App\" button (with a blue dot) to view the rendering. The environment only shows through the duration of the while loop. If the window closes, you can rerun this section (Ctrl+Enter), and click on the remote app button, to view the display again."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Trigger Ed's X display\n",
                "!xdpyinfo\n",
                "\n",
                "episodes = 10\n",
                "# Repeat process 10 times\n",
                "for episode in range(1, episodes+1):\n",
                "    # Each time, reset the environment\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    score = 0 \n",
                "    \n",
                "    while not done:\n",
                "        # render the environment so that it remains visible on the screen\n",
                "        env.render()  \n",
                "        # take a random choice to move left or right     \n",
                "        action = random.choice([0,1]) \n",
                "        # apply the action to the environment and collect feedback\n",
                "        n_state, reward, done, info = env.step(action) \n",
                "        # Add the reward to the cummulative score\n",
                "        score+=reward \n",
                "    # End of loop: print out the maximum score\n",
                "    print('Episode:{} Score:{}'.format(episode, score))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Create a Deep Learning Model with Keras"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import dependencies needed for this step from numpy and keras\n",
                "import numpy as np\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Flatten\n",
                "from tensorflow.keras.optimizers import Adam"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function that builds a model so that we can reuse it multiple times\n",
                "# To build a model, the function needs the available states and actions\n",
                "def build_model(states, actions):\n",
                "    model = Sequential()\n",
                "    model.add(Flatten(input_shape=(1,states)))\n",
                "    model.add(Dense(24, activation='relu'))\n",
                "    model.add(Dense(24, activation='relu'))\n",
                "    model.add(Dense(actions, activation='linear'))\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an instance of a model by calling the build_model function\n",
                "model = build_model(states, actions)\n",
                "# TODO: Write code to inspect the built model by outputting the summary\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Build Agent with Keras-RL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import dependencies to build an agent\n",
                "from rl.agents import DQNAgent\n",
                "from rl.policy import BoltzmannQPolicy\n",
                "from rl.memory import SequentialMemory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function to build a DQN agent given the model and the set of actions\n",
                "def build_agent(model, actions):\n",
                "    policy = BoltzmannQPolicy()\n",
                "    memory = SequentialMemory(limit=50000, window_length=1)\n",
                "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
                "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
                "    return dqn"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use the DQN agent to train the reinforcement learning model.\n",
                "\n",
                "Note that this step takes a few minutes. Move to the next steps after the button on the left changes from 'stop' to show that the run is complete. Do not worry about the 'too much output' warning halfways through the run.\n",
                "\n",
                "**To test this step**, please use the *Run All* button instead of running this section alone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an instance of an agent \n",
                "dqn = build_agent(model, actions)\n",
                "# Compile the model\n",
                "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
                "# Fit the model\n",
                "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Test Agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
                "print(np.mean(scores.history['episode_reward']))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Visualize the DQN model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "_ = dqn.test(env, nb_episodes=15, visualize=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Reload Agent from Memory"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use the save weights method to save the RL model weights in a file that will be saved under the ReinforcementLearning folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "dqn.save_weights('ReinforcementLearning/dqn_weights.h5f', overwrite=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Delete the model, agent and environment now that the weights are saved"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": [
                "del model \n",
                "del dqn\n",
                "del env"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Reinstanciate the env, model and dqn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = gym.make('CartPole-v0')\n",
                "actions = env.action_space.n\n",
                "states = env.observation_space.shape[0]\n",
                "model = build_model(states, actions)\n",
                "dqn = build_agent(model, actions)\n",
                "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Reload the weights into the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "dqn.load_weights('ReinforcementLearning/dqn_weights.h5f')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Earn Your Wings\n",
                "Test the environment again to see if we get similar results as before reloading the agent from memory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Write code to test the new model with reloaded weights\n",
                ""
            ]
        }
    ]
}
